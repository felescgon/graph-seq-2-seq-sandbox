import torch
import torch.nn as nn
import torch.optim as optim
from helpers import index_splitter
from helpers import make_balanced_sampler
from data_load import get_alibaba_machine_usage
from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset
from Discriminator import Discriminator
from Generator import Generator
from trainer import StepByStep
from sklearn.preprocessing import StandardScaler
import numpy as np

def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)

seq_len = 64 # No se puede cambiar sin cambiar la estructura convolucional del generador
batch_size = 16
x, y = get_alibaba_machine_usage(5120, seq_len)
train_idx, val_idx = index_splitter(len(x), [80,20])

x_tensor = torch.as_tensor(x)
y_tensor = torch.as_tensor(y)

x_train_tensor = x_tensor[train_idx]
y_train_tensor = y_tensor[train_idx]

#scale x
reshaped_x_train_tensor = x_train_tensor.reshape(-1,1)

scaler = StandardScaler(with_mean=True, with_std=True)
scaler.fit(reshaped_x_train_tensor)

scaled_x_train_tensor = torch.as_tensor(scaler.transform(reshaped_x_train_tensor).reshape(x_train_tensor.shape))

x_val_tensor = x_tensor[val_idx]
y_val_tensor = y_tensor[val_idx]

reshaped_x_val_tensor = x_val_tensor.reshape(-1,1)
scaled_x_val_tensor = torch.as_tensor(scaler.transform(reshaped_x_val_tensor).reshape(x_val_tensor.shape))


#making the samples
#sampler = make_balanced_sampler(y_train_tensor)

train_dataset = TensorDataset(scaled_x_train_tensor.float(), y_train_tensor.view(-1, 1).float())
test_dataset = TensorDataset(scaled_x_val_tensor.float(), y_val_tensor.view(-1, 1).float())

#train_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=sampler)
#The examples should be uniformly distributed among classes
train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size)

num_layers = 2
rnn_layer = nn.LSTM
bidirectional = False
n_discriminator_outputs = 1
n_features = x_train_tensor.shape[2]
hidden_dim_discriminator = 8

batch_norm = True
dropout = 0.2

torch.manual_seed(21)
generator = Generator(sequence_length=seq_len, n_features = n_features)
generator.apply(weights_init)
discriminator = Discriminator(n_features=n_features, hidden_dim=hidden_dim_discriminator, n_outputs=n_discriminator_outputs, num_layers=num_layers, bidirectional=bidirectional, rnn_layer=rnn_layer, batch_norm=batch_norm, dropout=dropout)
loss = nn.BCELoss()
generator_optimizer = optim.Adam(generator.parameters(), lr=0.0002)
discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)

sbs_rnn = StepByStep(generator, discriminator, loss, generator_optimizer, discriminator_optimizer)
sbs_rnn.set_loaders(train_loader, test_loader)
print("antes de entrenamiento")
sbs_rnn.train(20)
print("despu√©s de entrenamiento")

fig = sbs_rnn.plot_losses()
fig.show()
# accuracy_matrix = (StepByStep.loader_apply(test_loader, sbs_rnn.correct))
# print(accuracy_matrix)
# accuracy = [row[0]/row[1]for row in accuracy_matrix]
# print(f'Total accuracy: {np.mean(accuracy)*100} %')
noise = torch.randn(batch_size, 1, 1, device='cuda' if torch.cuda.is_available() else 'cpu').permute((0, 2, 1))
fake_data = sbs_rnn.predict(batch_size)
fake_data_rescaled = np.reshape(scaler.inverse_transform(fake_data.reshape(-1,1)), fake_data.shape)
np.savetxt(f'data/alibaba/generated/batch_{0}.csv', fake_data_rescaled[0], delimiter=",")
